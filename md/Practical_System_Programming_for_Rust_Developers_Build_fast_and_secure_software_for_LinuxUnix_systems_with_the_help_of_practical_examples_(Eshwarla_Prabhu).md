#### Practical System Programming for Rust Developers: Build fast and secure software for Linux/Unix systems with the help of practical examples (Eshwarla, Prabhu)
      Lexers and parsers are concepts used in computer science to build compilers and interpreters. A lexer (also called a tokenizer) splits text (source code) into words and assigns a lexical meaning to it such as keyword, expression, operator, function call, and so on. Lexers generate tokens (hence the name tokenizer).

      A parser takes the output of the lexer and arranges the tokens into a tree structure (a tree is a type of data structure). Such a tree structure is also called an AST. With the AST, the compiler can generate machine code and the interpreter can evaluate an instruction. Figure 2.7 of this chapter shows an illustration of an AST.

