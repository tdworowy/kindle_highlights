#### The Art of Statistical Thinking: Detect Misinformation, Understand the World Deeper, and Make Better Decisions. (Advanced Thinking Skills Book 2) (Rutherford, Albert)
      The way we select the sample is critically important, and it depends largely on the purpose of the study or the aim of the statistical task at hand.

      A sample that is a good representation of the population can be obtained by pure random sampling.

      The average of all the squared distances from the mean is called the variance,

      The sampling distributions of the sample mean shrink around the population mean of 0, as the sample size increases. That is, as the sample size increases, the accuracy of the sample means as an estimate of the population mean gets higher. This is the fundamental property that modern statistical methods rely heavily on.

      Hence, the Z-statistic can be interpreted as a (scaled) signal-to-noise ratio: how much is the signal from the sample away from the value of μ per unit of noise. If this value is close to 0, it means the sample value is a good indicator for the population value of μ, when the sampling noise is considered. If this value is far away from 0, the sample value is a poor indicator for the population value of , when the sampling noise is considered.

      Many problems we are going to see in a later chapter, such as the correlation vs. causation,  conflict between practical importance and statistical importance, and  misinterpretation of statistics  come down to this problem. Hence, a good statistical thinker should be able to distinguish the effect size from the test statistic and make a statistical decision based on effect size in combination with the test statistic. Sounds simple, but many top-level professional statisticians often fail to do so.

      Due to its simplicity, the p-value is widely reported in many statistical reports or articles but sometimes misused or abused. For example, a low p-value can tell us the sample is not compatible with the null hypothesis, but it does not necessarily mean the effect or signal is practically large. However, it is often the case that a low p-value is misinterpreted as the indication of strong effect. The readers should be reminded that a low p-value can be associated with practically negligible effect or signal.

      The problems include, the researchers do not fully consider the statistical uncertainty related with Type I and II errors,  they often ignore the effect size or the signal from the sample, and  they set the decision threshold arbitrarily with no scientific or practical justifications.

      Here there are two important points to note: There is a trade-off between the chance of a Type I error and the chance of a Type II error;  The decision threshold should be adjusted to reflect the consequences of Type I and II errors and their chances.

      The conventional hypothesis testing, however, almost always chooses the value of alpha at 0.05 or sometimes at 0.01 or 0.10. That means, in the above setting, the researcher is allowing for a relatively high value of β and a low value of statistical power. Are there any good reasons for this, not to mention scientific justification? The answer is absolutely no. Conventional hypothesis testing is not based on such reasoning or scientific justification.

      larger sample size will deflate the p-value, and it will become more and more likely to reject the null hypothesis as the sample size gets larger.

      Hence, the two important factors that control the power and Type II error probability at a given value of Type I probability are the value under H1, and  the sample size.

      The implication is that, with a large enough sample size, any negligible deviation from the value under H0 can be shown to be statistically significant, as the above example shows.

      Fisher further stated, “No scientific worker has a fixed level of significance at which from year to year, and in all circumstances, he rejects hypotheses; he rather gives his mind to each particular case in the light of his evidence and his ideas.”

      If the p-value is not less than 0.05, they believe they might have done something wrong, and they keep trying (data collection, different models, or methods, etc.) until it becomes less than 0.05. This process is called p-hacking or data snooping as we shall see in a later chapter. Hence, the readers of academic journals and statistical reports only see a distorted picture of research outcomes (called publication bias).

      As the famous statistician George Box stated in 1976, “all models are wrong, but some are useful.”

      As we shall see in the next chapter, the replication rate in scientific research is less than 50%. That means that a statistical study cannot be replicated by other researchers in most cases. This is called the replication crisis in scientific research, which will be discussed in more detail in the next chapter.

      Is the stock market efficient? This is a big question for many professionals in the fund management and stock trading industry, as well as lay investors. If the market is (perfectly) efficient, all prices fully and instantly reflect all available information, and no investor can make abnormal profits consistently. If the market is inefficient, the price under-reacts or over-reacts to the desired level implied by the available information with a delay, and there is room for abnormal profits for those investors who can time the market.

      However, one thing is clear. Academics proved that perfect and absolute efficiency is impossible because, if the price really reflects all information and makes accurate adjustments instantly to the desired level, then there is no motivation to trade. In response to this argument, a modified version of the efficient market hypothesis is proposed, which is called the adaptive markets hypothesis. [xiv]

      Some argue the market is adaptive and changes continuously depending on the prevailing market and economic conditions. The state of the market cannot be dichotomous between being efficient or inefficient; rather the market can depart from these two extreme states and show market inefficiency or return predictability occasionally.

      Broadly, there are two group of factors: macroeconomic factors and style factors. The former includes the key macroeconomic indicators, such as economic growth, inflation rate, and interest rate, and the latter includes the size, value, momentum, quality, and volatility.

      Statistics are widely being misinterpreted, misused, and abused. It is rampant in scientific studies, media reports, business decisions, and government policymaking.

      A statistically significant correlation can occur as a result of pure coincidence, as the following example shows: the number of drowning deaths in the U.S. and the number of movies in which Nicolas Cage appeared.

      This has created the serious problem of publication bias or the file drawer problem, where only the studies with statistically significant results are published in academic journals. Many meta-analytic studies report evidence of publication bias. A survey in empirical finance reports that 98% of the published papers come with statistical significance (Kim and Ji, 2014). More serious is the file drawer problem, where potentially important research findings are not published simply because they have not achieved statistical significance.

      Statistical insignificance, as well as statistical significance, should be evaluated by combining all available information, not solely on the magnitude of the Z-statistics or p-value. It should be considered in association with the plausibility of the hypothesis, the soundness of the research design, and effect size.

